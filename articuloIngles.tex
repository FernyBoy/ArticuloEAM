\documentclass[conference]{IEEEtran} % puedes cambiar a article si prefieres

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

%% IEEE Trans parece requerir fontenc
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{orcidlink}

\newcommand{\affmark}[1]{\textsuperscript{#1}}
\newcommand{\affaddr}[1]{\textit{#1}}

\usepackage{fancyhdr}
\fancypagestyle{firststyle}
{
	\fancyhf{}
	\fancyhead[C]{\fontsize{10}{10} \selectfont \textit{2025 Mexican International Conference on Computer Science (ENC) \& IEEE ICEV, Orizaba, Veracruz, México} }
	\fancyfoot[L]{\fontsize{8}{10} \selectfont \textbf{979-8-3315-9026-0/25/\$31.00 \textcopyright 2025 IEEE}}
	\fancyfoot[R]{\fontsize{8}{10} \selectfont \textbf{Online ISSN: 2332-5712}}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}
	
\title{Large-Scale Validation and Analysis of the Rejection Capacity in Entropic Associative Memory}

\thanks{The work was conducted during the first author’s summer stay at the University of Guadalajara, supported by a scholarship from the Programa Delfín.}

\author{%
\IEEEauthorblockN{%
	Angel Fernando Bórquez-Guerrero,
}%
\IEEEauthorblockA{%
	\textit{Campus Hermosillo}\\
	\textit{Universidad de Sonora}\\
	Hermosillo, Sonora, Mexico\\
	fernandoborquez215@gmail.com
}%
\and
\IEEEauthorblockN{%
	Rafael Morales-Gamboa,
}%
\IEEEauthorblockA{%
	\textit{Centro Universitario de Guadalajara}\\
	\textit{Universidad de Guadalajara}\\
	Guadalajara, Jalisco, Mexico\\
	\orcidlinkc{0000-0003-4347-6028}
}%
}

\maketitle
\thispagestyle{firststyle}
\renewcommand{\headrulewidth}{0in}

% -------------------- Abstract --------------------
\begin{abstract}
The Entropic Associative Memory (EAM), proposed by Pineda and Morales (2023), is a computational model inspired by human cognition that combines probabilistic recognition properties with an efficient architecture for pattern storage and retrieval. In previous studies, EAM was evaluated using the Fashion MNIST dataset, yielding positive results in image classification and reconstruction, although limited to a reduced domain of only ten classes.

This work presents two main contributions aimed at overcoming these limitations. First, we assess the scalability of the model by migrating to a substantially larger dataset: Google’s \emph{Quick, Draw!}, which contains more than 300 classes and hundreds of thousands of examples per category. Second, we design a rejection experiment to verify whether the memory is capable of identifying and not recognizing those classes that were not explicitly stored. To this end, the memory is trained with only half of the classes and evaluated on the complete set.

The results confirm that EAM can adapt to more complex scenarios and suggest that it may provide an effective rejection mechanism against novel stimuli, offering evidence of its potential as an explainable and robust model within the field of artificial intelligence.
\end{abstract}

% -------------------- Keywords --------------------
\begin{IEEEkeywords}
Entropic Associative Memory, Quick Draw, Fashion MNIST, scalability, open-set recognition
\end{IEEEkeywords}



% -------------------- Introducción --------------------
\section{Introduction}

\noindent The development of memory models inspired by human cognition has become a rapidly growing research area within artificial intelligence (AI) \cite{russell2020artificial}. These approaches aim to computationally reproduce the ability of biological systems to store, associate, and retrieve information from past experiences. In this context, the \textit{Entropic Associative Memory} (EAM), proposed by Pineda and Morales \cite{pinedaImageryEntropicAssociative2023}, represents a significant theoretical advancement by providing a declarative, distributed, and efficient architecture capable of probabilistic recognition and flexible pattern retrieval.

Initial studies with EAM focused on classical and relatively small datasets such as \textit{Fashion MNIST} \cite{zalandoresearchFashionMNIST2022}, which contains 70,000 images across 10 object classes. In this domain, the model demonstrated the ability to store latent representations generated by a convolutional autoencoder and to retrieve these representations even in the presence of noise or incomplete information. These results validated the effectiveness of the model in closed-set recognition tasks, where all test classes (though not all instances) had been previously stored in memory.

% ***** "There are undefined references in the introduction section. The text mentions three main limitations to the experimental framework from [1], but only two are stated" *****
However, the original experimental framework presented three main limitations. First, the evaluation was carried out in a small and limited domain, restricting the analysis of the model’s scalability. Second, the memory’s ability to explicitly reject instances from non-stored classes had not been tested, a critical aspect in \emph{open-set recognition} problems \cite{scheirerOpenSetRecognition2013}. Finally, the experimental design was tightly coupled to the fixed number of classes configured in the neural networks, limiting flexibility.
% *************************************************************************************

This work seeks to overcome these limitations through two lines of research: (1) evaluating the scalability of EAM using the \textit{Quick, Draw!} dataset \cite{googlecreativelabQuickDrawData2025}, which contains more than 300 classes and hundreds of thousands of examples per category; and (2) designing a rejection experiment to examine the memory’s performance in not recognizing non-stored classes.

In this way, the article contributes to broadening the understanding of the behavior of the Entropic Associative Memory in more complex scenarios, providing empirical evidence of its robustness, generalization capacity, and potential as an alternative paradigm within explainable artificial intelligence.



% -------------------- Antecedentes --------------------
\section{Background}

\noindent The \textit{Entropic Associative Memory} (EAM), proposed by Pineda and Morales \cite{pinedaImageryEntropicAssociative2023}, emerges as a computational model inspired by human cognition, designed to probabilistically and distributively store and retrieve patterns. Unlike conventional memory systems, EAM operates on a single \textit{Associative Memory Register}, which concentrates the associations of all learned patterns in a matrix representation. This approach enables flexible behavior in recognition and retrieval and opens the door to the study of emergent phenomena such as generalization and synthetic image generation.

The model is based on three main operations: 
\begin{itemize}
    \item \textbf{$\lambda$-register}: records a pattern in memory by reinforcing associations among its features.
    \item \textbf{$\eta$-recognition}: determines whether an input stimulus is recognized by the memory, using error tolerance criteria and confidence thresholds.
    \item \textbf{$\beta$-retrieval}: retrieves a complete pattern from a cue, which may be partial or noisy, by leveraging probabilistic distributions over the stored associations.
\end{itemize}

% ***** "More details on how the EAM model works and its characteristics as an explainable model, such as the types of explanations it generates, will be helpful to the reader" *****
A key characteristic of the EAM, which contributes to its potential as an explainable AI (XAI) model, is the transparency of its memory representation. Since the \textit{Associative Memory Register} is a matrix where all associations are stored, its state can be directly inspected and visualized. This allows for a clear understanding of how the memory is being populated and which features are being associated. Consequently, when the memory performs a recognition or retrieval operation, it is possible to trace back the process and analyze the strength of the associations that led to the outcome, providing a clear explanation for the model's behavior, in contrast to the "black box" nature of many deep learning models.
% *****************************************************************************************************************

In previous experiments, EAM was evaluated with the \textit{Fashion MNIST} dataset \cite{zalandoresearchFashionMNIST2022}, which consists of 70,000 images across 10 classes. In this setting, the memory was supplied with latent representations generated by a convolutional autoencoder and demonstrated the ability to both recognize and reconstruct patterns even under noisy or incomplete conditions. Furthermore, emergent phenomena such as the generation of memory sequences (\textit{association chains}) were explored, showing the system’s capacity to transition between stored representations. A subsequent study was carried out using Google’s \emph{Quick, Draw!} dataset, with small variations in the number of classes (between 10 and 13), yielding acceptable results \cite{gonzalez2024clasificador}.

Despite these promising results, the original experimental framework presented relevant limitations:  
(1) the evaluation was restricted to a reduced set of 10 classes, which hindered the analysis of model scalability in more complex contexts;  
(2) no systematic tests were conducted on the memory’s ability to reject patterns from unseen classes, a fundamental aspect in open-set recognition; and  
(3) the experimental design was tightly coupled to the fixed number of classes configured in the neural networks, limiting flexibility in exploring different memory-loading configurations.

These limitations motivate the present work, whose goal is to validate EAM in a more challenging environment through migration to the \textit{Quick, Draw!} dataset \cite{googlecreativelabQuickDrawData2025} and the design of a specific experiment to analyze its rejection capability against unseen classes.



% -------------------- Metodología --------------------
\section{Methodology}

\noindent To extend the validation of the Entropic Associative Memory (EAM) model and analyze its behavior in more complex scenarios, an experimental methodology was designed and divided into three main phases. These phases enabled migration to a new data domain, evaluation of the explicit rejection mechanism, and provided the system with greater flexibility for future experiments.

\subsection{Dataset Migration and Large-Scale Validation}
\noindent The first step consisted of replacing the \textit{Fashion MNIST} dataset with \textit{Quick, Draw!} \cite{googlecreativelabQuickDrawData2025}, which contains more than 300 classes and hundreds of thousands of examples per category. This process required:
\begin{itemize}
    \item \textbf{Adaptation of the data module:} The \texttt{dataset.py} script was modified to load, process, and dynamically balance \emph{Quick, Draw!} classes. The logic was adjusted to handle a variable number of categories and to guarantee an equal number of samples per class.
    \item \textbf{Retraining of neural networks:} The perceptual system, consisting of an autoencoder and a classifier (defined in \texttt{neural\_net.py}), was adjusted based on the work in \cite{moralesMissingCueProblem2025} and retrained from scratch using \emph{Quick, Draw!} as the new data source. This step produced suitable latent representations (\textit{features}) for the memory.
    \item \textbf{Pipeline validation:} The full workflow (autoencoder-classifier training, feature generation, memory storage, and recognition evaluation) was executed to verify the model’s compatibility with the new domain.
\end{itemize}

\subsection{Design and Implementation of the Rejection Experiment}
\noindent To analyze the system’s ability to reject patterns from unseen classes, a second experiment was implemented with the following logic:
\begin{itemize}
    \item \textbf{Extension of the command interface:} A new argument was added to the main program \texttt{eam.py} (\texttt{-e 2}) to invoke this experimental mode.
    \item \textbf{Partial memory registration:} Although the classifier was trained with all selected classes, only half of them were stored in EAM.
    \item \textbf{Evaluation with the complete set:} During the test phase, the system was exposed to all classes, including those not stored, enabling explicit evaluation of its rejection response.
    \item \textbf{Acceptance and rejection metric:} The \texttt{rejection\_response} index was employed as a quantitative measure to evaluate the system’s ability to avoid associating patterns from unseen classes.
\end{itemize}

\subsection{Refactoring for Experimental Flexibility}
\noindent To transform the system into a more versatile experimental platform, additional modifications were carried out:
\begin{itemize}
    \item \textbf{Parameterization of the number of classes:} The \texttt{eam.py} program was modified to accept the number of classes as a runtime argument (\texttt{<classes>}), instead of being hardcoded.
    \item \textbf{Parameter propagation:} This value was passed to the data module, allowing dynamic preparation of only the required subset of classes.
    \item \textbf{Dynamic loading:} The system was enabled to run experiments with different numbers of classes (2, 4, 8, 16, 24, or 32), adapting to controlled scenarios of varying complexity.
\end{itemize}
%
Taken together, these three methodological phases allowed the model to scale to a substantially larger domain, analyze its rejection capability when facing novel stimuli, and provide a more flexible infrastructure for future experimental trials.



% -------------------- Resultados --------------------
\section{Results}

\subsection{Quantitative Results by Number of Classes}
% ***** "The limitation of considering only 64 of the 300 classes in the dataset must be noted when discussing the findings" *****
\indent This section presents the performance of the autoencoder-classifier and the entropic associative memory as the number of classes varies. Out of the 345 classes in the \emph{Quick, Draw!} dataset, a total of 64 were randomly selected for the experiments. From this set, experiments were run using the first 2, 4, 8, 16, 24, and 32 classes. For each configuration, the dataset was balanced to contain 12,000 samples per class, which were divided into three subsets: 10,000 for the autoencoder-classifier training corpus, 1,000 for the entropic associative memory's filling corpus, and 1,000 for the test corpus used by both. Due to time constraints and the memory limitations of the server, experiments with more classes were not performed. However, the selection of 64 classes was considered sufficient to observe a clear trend in the results, and the consistency of the results suggests that they are not far from what would be obtained with cross-validation.
% ********************************************************************************************************************************

As in \cite{pinedaImageryEntropicAssociative2023} and \cite{moralesMissingCueProblem2025}, the neural networks implementing the autoencoder and the classifier were trained jointly. Table~\ref{tab:autoencoder-classifier-performance} presents their performance on the test set for the different numbers of classes taken from the dataset. As can be observed, the classifier’s performance tends to decrease as the number of classes increases, but it remained above 91\% accuracy. The autoencoder’s performance is slightly more stable, with a difference of less than 3\% across all cases.

\begin{table}[htbp]
	\centering
	\caption{Performance of the autoencoder (root mean square error) and the classifier (accuracy) for different numbers of classes.}
	\begin{tabular}{@{}ccc@{}}\toprule
		Number of Classes & Accuracy &  Error \\
		\midrule
		2 & 0.976 & 0.218 \\
		4 & 0.950 & 0.215 \\
		8 & 0.921 & 0.204 \\
		16 & 0.914 & 0.218 \\
		24	& 0.917 & 0.228 \\
		32 & 0.917 & 0.229 \\
		\bottomrule
	\end{tabular}
	\label{tab:autoencoder-classifier-performance}
\end{table}

To facilitate a clearer comparison of the results, Table~\ref{tab:summary-performance} provides a summary of the key performance metrics across the different experimental setups. The table shows the baseline classifier accuracy alongside the estimated peak accuracy of the Entropic Associative Memory in both the full recognition and the rejection experiments.

% ***** "The presentation of the results can be complemented with a table for easier comparison" *****
\begin{table}[htbp]
	\centering
	\caption{Summary of performance across experiments.}
	\begin{tabular}{@{}cccc@{}}\toprule
		Number of & Classifier & EAM Accuracy & EAM Accuracy \\
		Classes & Accuracy & (Recognition Exp.) & (Rejection Exp.) \\
		\midrule
		2 & 0.976 & $\approx$0.97 & $\approx$0.95 \\
		4 & 0.950 & $\approx$0.95 & $\approx$0.85 \\
		8 & 0.921 & $\approx$0.92 & $\approx$0.75 \\
		16 & 0.914 & $\approx$0.91 & $\approx$0.65 \\
		24	& 0.917 & $\approx$0.91 & $\approx$0.60 \\
		32 & 0.917 & $\approx$0.91 & $\approx$0.55 \\
		\bottomrule
	\end{tabular}
	\label{tab:summary-performance}
\end{table}
% ******************************************************************




\subsection{Recognition Experiment Results (All Classes Stored in Memory)}
\noindent In this experiment, the memory stored all the available classes, which were also used to train and evaluate the neural networks. Figures~\ref{fig:precision_2} to \ref{fig:precision_32} show the corresponding performance of the entropic associative memory using precision and accuracy metrics. In all cases, the same number of memory columns (256) was kept, while the number of rows was varied in powers of two, from 2 up to 1024. Overall, it can be observed that the performances with 4 or more rows were very similar to each other and comparable to those obtained by the classifier.


% -- 2 Classes
% ***** "In Figures 1-6, it is not clear what the entropy bar at the bottom of the figures is for. More details or a brief description will be helpful for the reader" *****
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_02/graph_prse_MEAN-english.pdf}
    \caption{Precision and accuracy of the associative memory with 2 classes. The color bar at the bottom represents the entropy of the memory, where blue indicates low entropy (high certainty) and red indicates high entropy (high uncertainty).}
    \label{fig:precision_2}
\end{figure}

% -- 4 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_04/graph_prse_MEAN-english.pdf}
    \caption{Precision and accuracy of the associative memory with 4 classes. The color bar at the bottom represents the entropy of the memory, where blue indicates low entropy (high certainty) and red indicates high entropy (high uncertainty).}
    \label{fig:precision_4}
\end{figure}

% -- 8 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_08/graph_prse_MEAN-english.pdf}
    \caption{Precision and accuracy of the associative memory with 8 classes. The color bar at the bottom represents the entropy of the memory, where blue indicates low entropy (high certainty) and red indicates high entropy (high uncertainty).}
    \label{fig:precision_8}
\end{figure}

% -- 16 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_16/graph_prse_MEAN-english.pdf}
    \caption{Precision and accuracy of the associative memory with 16 classes. The color bar at the bottom represents the entropy of the memory, where blue indicates low entropy (high certainty) and red indicates high entropy (high uncertainty).}
    \label{fig:precision_16}
\end{figure}

% -- 24 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_24/graph_prse_MEAN-english.pdf}
    \caption{Precision and accuracy of the associative memory with 24 classes. The color bar at the bottom represents the entropy of the memory, where blue indicates low entropy (high certainty) and red indicates high entropy (high uncertainty).}
    \label{fig:precision_24}
\end{figure}

% -- 32 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_32/graph_prse_MEAN-english.pdf}
    \caption{Precision and accuracy of the associative memory with 32 classes. The color bar at the bottom represents the entropy of the memory, where blue indicates low entropy (high certainty) and red indicates high entropy (high uncertainty).}
    \label{fig:precision_32}
\end{figure}
% ***********************************


% --------------------
\noindent To observe the performance of the memory as it is filled with memories, for each number of classes, a set of rows with good performance was selected, and the behavior of the memory's precision and accuracy was observed as it was filled in percentages that also varied in powers of two. Figures \ref{fig:recall_exp1_2_004} to \ref{fig:recall_exp1_32_016} present the obtained results. In general, it can be stated that the memory's performance remains stable when filled with at least 8\% of the filling corpus.



% -- 2 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_02/recall-exp_001-sze_004-graph_prse_MEAN-english.pdf}
    \caption{Results of the recognition experiment with 2 classes and a memory of 4 rows.}
    \label{fig:recall_exp1_2_004}
\end{figure}

% -- 4 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_04/recall-exp_001-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the recognition experiment with 4 classes and a memory of 32 rows.}
    \label{fig:recall_exp1_4_032}
\end{figure}

% -- 8 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_08/recall-exp_001-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the recognition experiment with 8 classes and a memory of 16 rows.}
    \label{fig:recall_exp1_8_016}
\end{figure}

% -- 16 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_16/recall-exp_001-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the recognition experiment with 16 classes and a memory of 16 rows.}
    \label{fig:recall_exp1_16_016}
\end{figure}

% -- 24 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_24/recall-exp_001-sze_032-graph_prse_MEAN-english.pdf}
    \caption{Results of the recognition experiment with 24 classes and a memory of 32 rows.}
    \label{fig:recall_exp1_24_032}
\end{figure}

% -- 32 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_32/recall-exp_001-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the recognition experiment with 32 classes and a memory of 16 rows.}
    \label{fig:recall_exp1_32_016}
\end{figure}



% --------------------




\subsection{Results of the Experiment on Rejection of Unregistered Classes}
\noindent In this experiment, only the first half of the classes were stored in the memory, but evaluation was performed against the complete set. For each number of classes, the same number of memory rows was selected as in the previous experiment (Figures \ref{fig:recall_exp1_2_004} to \ref{fig:recall_exp1_32_016}), and the performance of the memory was observed as it was being filled. Figures \ref{fig:recall_exp2_2_004} to \ref{fig:recall_exp2_32_016} present the corresponding precision and accuracy graphs. It can be observed that the memory performs very well with two classes and little information stored, and its performance decreases as both the number of classes and the number of elements from the filling corpus stored in memory increase.

% -- 2 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_02/recall-exp_002-sze_004-graph_prse_MEAN-english.pdf}
    \caption{Results of the rejection experiment with 2 classes and a memory of 4 rows.}
    \label{fig:recall_exp2_2_004}
\end{figure}

% -- 4 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_04/recall-exp_002-sze_032-graph_prse_MEAN-english.pdf}
    \caption{Results of the rejection experiment with 4 classes and a memory of 32 rows.}
    \label{fig:recall_exp2_4_032}
\end{figure}

% -- 8 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_08/recall-exp_002-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the rejection experiment with 8 classes and a memory of 16 rows.}
    \label{fig:recall_exp2_8_016}
\end{figure}

% -- 16 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_16/recall-exp_002-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the rejection experiment with 16 classes and a memory of 16 rows.}
    \label{fig:recall_exp2_16_016}
\end{figure}

% -- 24 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_24/recall-exp_002-sze_032-graph_prse_MEAN-english.pdf}
    \caption{Results of the rejection experiment with 24 classes and a memory of 32 rows.}
    \label{fig:recall_exp2_24_032}
\end{figure}

% -- 32 Classes
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Graficas/runs_32/recall-exp_002-sze_016-graph_prse_MEAN-english.pdf}
    \caption{Results of the rejection experiment with 32 classes and a memory of 16 rows.}
    \label{fig:recall_exp2_32_016}
\end{figure}



% -------------------- Discución --------------------
\section{Discussion}
\noindent The experimental results allow for several relevant observations about the behavior of the Entropic Associative Memory (EAM) in more complex scenarios.

Firstly, by progressively increasing the number of classes, a general trend of decreasing precision and recall metrics was observed. This behavior is expected, as the domain complexity and pattern variability grow with the number of categories. Nevertheless, the EAM maintained stable performance in low and medium-complexity configurations, which confirms its ability to scale beyond the reduced Fashion MNIST domain.

Secondly, the rejection experiments showed that the memory was capable of identifying patterns corresponding to classes registered within it and rejecting instances of unregistered classes. This result provides empirical validation for the hypothesis that EAM can operate in open-set recognition scenarios, differentiating between familiar and unfamiliar stimuli. However, cases of false acceptances for instances of unregistered classes were also detected as the number of classes, or the percentage of the corpus filled in the memory, increased. This suggests a need to adjust threshold parameters ($\iota$ and $\kappa$ \cite{pinedaImageryEntropicAssociative2023,moralesMissingCueProblem2025}), try other row numbers, or explore complementary discrimination mechanisms.

Finally, the comparison with the baseline classifier highlights the role of the memory in the complete system. While the classifier provides useful latent representations, it is the EAM that confers the probabilistic rejection and retrieval capability, providing added value compared to purely discriminative architectures.

Collectively, these findings demonstrate that EAM can not only adapt to more complex domains but also offers an effective mechanism for handling novel information. At the same time, they highlight future challenges related to optimizing its parameters and evaluating it in even larger and more heterogeneous domains. It is important to note, however, that these conclusions are based on experiments with only 64 of the 345 available classes, and future work should aim to scale these tests to the entire dataset.


% -------------------- Conclusiones --------------------
\section{Conclusions}

\noindent This work extended the experimental framework of the Entropic Associative Memory (EAM) to evaluate its performance in more complex scenarios and analyze its rejection capacity for unregistered classes. The main contributions can be summarized as follows:

\begin{itemize}
    \item \textbf{Large-scale validation:} We migrated from a reduced domain of ten classes in Fashion MNIST to a wider and more diverse domain using \emph{Quick, Draw!}, with results that confirm the model's ability to scale in contexts with much more classes and greater variability in patterns.
    
    \item \textbf{Rejection capacity:} A new experiment was designed and implemented, empirically showing how the EAM is capable of issuing a rejection to stimuli corresponding to unseen classes, validating its usefulness in open-set recognition problems.
    
    \item \textbf{Experimental flexibility:} The system was refactored to accept a variable number of classes and different memory sizes, which enables a more versatile environment for exploring configurations and comparative analysis.
\end{itemize}

\noindent The findings demonstrate that the EAM is an explainable and robust model, capable of combining distributed storage with probabilistic retrieval and rejection mechanisms. These results reinforce its potential as an alternative paradigm within artificial intelligence, particularly in tasks where the ability to handle novel information is critical.

For future work, it is proposed to extend the experiments to even larger and more heterogeneous domains, to optimize the threshold parameters associated with rejection, and investigate the integration of EAM with more advanced deep learning architectures to strengthen both its accuracy and generalization capacity.


% ***** Comentario del reviwer #2 *****
% -------------------- Reproducibility --------------------
\section*{Reproducibility}
\noindent The source code, experimental data, and scripts required to reproduce the results presented in this paper are available at a public repository: \url{https://github.com/FernyBoy/quick.git}. The main scripts modified for this work were \texttt{dataset.py} for loading and processing the \emph{Quick, Draw!} data, \texttt{neural\_net.py} for the autoencoder and classifier models, and \texttt{eam.py} for the main experimental logic and the implementation of the rejection experiment.
% *************************************


% -------------------- Referencias --------------------
\bibliographystyle{IEEEtran}
\bibliography{articulo}
\end{document}
