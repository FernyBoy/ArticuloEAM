\documentclass[conference]{IEEEtran} % puedes cambiar a article si prefieres

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

%% IEEE Trans parece requerir fontenc
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{orcidlink}
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi

\newcommand{\affmark}[1]{\textsuperscript{#1}}
\newcommand{\affaddr}[1]{\textit{#1}}

\usepackage{fancyhdr}
\fancypagestyle{firststyle}
{
	\fancyhf{}
	\fancyhead[C]{\fontsize{10}{10} \selectfont \textit{2025 Mexican International Conference on Computer Science (ENC) \& IEEE ICEV, Orizaba, Veracruz, México} }
	\fancyfoot[L]{\fontsize{8}{10} \selectfont \textbf{979-8-3315-9026-0/25/\$31.00 \textcopyright 2025 IEEE}}
	\fancyfoot[R]{\fontsize{8}{10} \selectfont \textbf{Online ISSN: 2332-5712}}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}
	
\title{Large-Scale Validation and Analysis of the Rejection Capacity in Entropic Associative Memory}

\thanks{The work was conducted during the first author's summer stay at the University of Guadalajara, supported by a scholarship from the Programa Delfín.}

\author{%
\IEEEauthorblockN{%
	Angel Fernando Bórquez-Guerrero,
}%
\IEEEauthorblockA{%
	\textit{Campus Hermosillo}\\
	\textit{Universidad de Sonora}\\
	Hermosillo, Sonora, Mexico\\
	\href{mailto:fernandoborquez215@gmail.com}{fernandoborquez215@gmail.com}
}%
\and
\IEEEauthorblockN{%
	Rafael Morales-Gamboa,
}%
\IEEEauthorblockA{%
	\textit{Centro Universitario de Guadalajara}\\
	\textit{Universidad de Guadalajara}\\
	Guadalajara, Jalisco, Mexico\\
	\orcidlinkc{0000-0003-4347-6028}
}%
}

\maketitle
\thispagestyle{firststyle}
\renewcommand{\headrulewidth}{0in}

% -------------------- Abstract --------------------
\begin{abstract}
The Entropic Associative Memory (EAM) is a computational model inspired by human cognition, combining probabilistic recognition with an efficient architecture for pattern storage and retrieval. Previous evaluations were limited to small-scale domains, typically containing ten classes and a few thousand instances per class, yet they showed positive results for EAM in classification and reconstruction. This work addresses these limitations with two main contributions. First, we assess the model's scalability by evaluating it on a substantially larger dataset: Google's \textit{Quick, Draw!}, which features over 300 classes and more than 100,000 instances per class. Second, we design a rejection experiment to test EAM's ability to reject instances from classes not explicitly stored in memory. To do so, the memory is filled with only half of the available classes and then evaluated on the complete dataset. The results confirm EAM's adaptability to more complex, large-scale scenarios. They also demonstrate an effective rejection mechanism for novel stimuli, highlighting its potential as a robust and explainable model within the field of artificial intelligence.\end{abstract}

% -------------------- Keywords --------------------
\begin{IEEEkeywords}
Entropic Associative Memory, Quick Draw, scalability, open-set recognition, rejection.
\end{IEEEkeywords}



% -------------------- Introducción --------------------
\section{Introduction}

\noindent The development of memory models inspired by human cognition has become a rapidly growing research area within artificial intelligence (AI) \cite{russellArtificialIntelligenceModern2020}. These approaches aim to computationally reproduce the ability of biological systems to store, associate, and retrieve information from past experiences. In this context, the \textit{Entropic Associative Memory} (EAM), proposed by Pineda and Morales \cite{pinedaImageryEntropicAssociative2023}, represents a significant theoretical advancement by providing a declarative, distributed, and efficient architecture capable of probabilistic recognition and flexible pattern retrieval.

Initial studies with EAM focused on classical and relatively small datasets, such as \textit{Fashion MNIST} \cite{zalandoresearchFashionMNIST2022}. In that domain, the model demonstrated its ability to store latent representations generated by a convolutional autoencoder and to retrieve these representations even in the presence of noise or incomplete information. These results validated the effectiveness of the model in closed-set recognition tasks, where all test classes (though not all instances) had been previously stored in memory.

% ***** "There are undefined references in the introduction section. The text mentions three main limitations to the experimental framework from [1], but only two are stated" *****
The original experimental framework presents three main limitations. First, the evaluation was carried out in small and limited domains, restricting the analysis of the model's scalability. Second, the memory's ability to explicitly reject instances from non-stored classes had not been tested, a critical aspect in \emph{open-set recognition} problems \cite{scheirerOpenSetRecognition2013}. Third, the experimental design was tightly coupled to the fixed number of classes configured in the neural networks, limiting flexibility.
% *************************************************************************************

This work seeks to overcome these limitations through two lines of research: (1) evaluating the scalability of EAM using the \textit{Quick, Draw!} dataset \cite{googlecreativelabQuickDrawData2025}, which contains more than 300 classes and hundreds of thousands of examples per category; and (2) designing a rejection experiment to examine the memory's performance in not recognizing non-stored classes.

In this way, the article contributes to broadening the understanding of the behavior of the Entropic Associative Memory in more complex scenarios, providing empirical evidence of its robustness, generalization capacity, and potential as an alternative paradigm within explainable artificial intelligence.



% -------------------- Antecedentes --------------------
\section{Background}

\noindent The entropic associative memory, proposed by Pineda and Morales \cite{pinedaImageryEntropicAssociative2023}, emerges as a computational model inspired by human cognition, designed to probabilistically and distributively store and retrieve patterns. Unlike conventional memory systems, EAM operates on a single associative memory register, which concentrates the associations of all learned patterns in a matrix representation. This approach enables flexible behavior in recognition and retrieval, and it opens the door to the study of emergent phenomena such as generalization and synthetic image generation.

The model is based on three main operations: 
\begin{itemize}
    \item \textbf{$\boldsymbol{\lambda}$-register}: records a pattern in memory by reinforcing associations among its features.
    \item \textbf{$\boldsymbol{\eta}$-recognition}: determines whether an input stimulus is recognized by the memory, using error tolerance criteria and confidence thresholds.
    \item \textbf{$\boldsymbol{\beta}$-retrieval}: retrieves a complete pattern from a cue, which may be partial or noisy, by leveraging probabilistic distributions over the stored associations.
\end{itemize}

% ***** "More details on how the EAM model works and its characteristics as an explainable model, such as the types of explanations it generates, will be helpful to the reader" *****
A key characteristic of the EAM, which contributes to its potential as an explainable AI model, is the transparency of its memory representation. Since the \textit{Associative Memory Register} is a matrix where all associations are stored, its state can be directly inspected and visualized. This allows for a clear understanding of how the memory is being populated and which features are being associated. Consequently, when the memory performs a recognition or retrieval operation, it is possible to trace back the process and analyze the strength of the associations that led to the outcome, providing a clear explanation for the model's behavior, in contrast to the ``black box'' nature of many deep learning models.
% *****************************************************************************************************************

In previous experiments \cite{pinedaImageryEntropicAssociative2023,hernandezRememberingCIFAR10Images2026}, EAM was evaluated with the \textit{Fashion MNIST} \cite{zalandoresearchFashionMNIST2022} and CIFAR-10 \cite{alexkrizhevskyLearningMultipleLayers2009} datasets, which consists of 70,000 and 60,000 images across 10 classes, respectively. In this setting, the memory was supplied with latent representations generated by a convolutional autoencoder and demonstrated the ability to both recognize and reconstruct patterns even under noisy or incompleteness conditions. Furthermore, emergent phenomena such as the generation of memory sequences (\textit{association chains}) were explored, showing the system's capacity to transition between stored representations. A subsequent study was carried out using Google's \emph{Quick, Draw!} dataset, with small variations in the number of classes (between 10 and 13), yielding acceptable results \cite{gonzalez2024clasificador}.

Despite its promising results, the original experimental framework presented relevant limitations:  
(1) the evaluation was restricted to a reduced set of 10--13 classes, which hindered the analysis of model scalability in more complex contexts;  
(2) no systematic tests were conducted on the memory's ability to reject patterns from unseen classes, a fundamental aspect in open-set recognition; and  
(3) the experimental design was tightly coupled to the fixed number of classes configured in the neural networks, limiting flexibility in exploring different memory-loading configurations.

These limitations motivate the present work, whose goals is to validate EAM in a more challenging environment through migration to Google's \textit{Quick, Draw!} dataset \cite{googlecreativelabQuickDrawData2025}, decoupling the number of classes used to train the neural networks and to fill the memory, and running a specific experiment to analyze its rejection capability against unseen classes.



% -------------------- Metodología --------------------
\section{Methodology}

\noindent To extend the validation of the EAM model, and analyze its behavior in more complex scenarios, an experimental methodology was designed and divided into three main phases. These phases enabled migration to a new data domain, evaluation of the explicit rejection mechanism, and provided the system with greater flexibility for future experiments.

\subsection{Dataset Migration and Large-Scale Validation}
\noindent The first step consisted in introducing the \textit{Quick, Draw!} dataset \cite{googlecreativelabQuickDrawData2025}. This process required:
\begin{itemize}
    \item \textbf{Adaptation of the data module:} The data module was modified to load, process, and dynamically balance an arbitrary subset of \emph{Quick, Draw!} classes. The logic was adjusted to handle a variable number of categories and to guarantee an equal number of samples per class.
    \item \textbf{Retraining of neural networks:} The perceptual system, consisting of an autoencoder and a classifier, was adjusted based on the work in \cite{moralesMissingCueProblem2025} and retrained from scratch using \emph{Quick, Draw!} as the new data source. This step produced suitable latent representations (\textit{features}) for the memory.
    \item \textbf{Pipeline validation:} The full workflow (autoencoder-classifier training, feature generation, memory storage, and recognition evaluation) was executed to verify the model's compatibility with the new domain.
\end{itemize}

\subsection{Design and Implementation of the Rejection Experiment}
\noindent To analyze the system's ability to reject instances of unseen classes, a second experiment was implemented with the following logic:
\begin{itemize}
    \item \textbf{Partial memory registration:} Although the perceptual system was trained with all selected classes, only half of them were stored in EAM.
    \item \textbf{Evaluation with the complete set:} EAM was exposed to all classes, including those not stored in it, enabling explicit evaluation of its rejection response.
    \item \textbf{Acceptance and rejection metric:} Standard metrics were employed to evaluate the system's ability to avoid associating patterns from unseen classes.
\end{itemize}

\subsection{Refactoring for Experimental Flexibility}
\noindent To transform the system into a more versatile experimental platform, additional modifications were carried out:
\begin{itemize}
    \item \textbf{Parametrization of the number of classes:} The main program was modified to accept the number of classes as a runtime argument, instead of being hard-coded.
    \item \textbf{Parameter propagation:} This value was passed to the data module, allowing dynamic preparation of only the required subset of classes.
    \item \textbf{Dynamic loading:} The system was enabled to run experiments with different numbers of classes, adapting to controlled scenarios of varying complexity.
\end{itemize}
%
Taken together, these three methodological phases allowed the model scaling to a substantially larger domain, analyze its rejection capability when facing novel stimuli, and provide a more flexible infrastructure for future experimental trials.



% -------------------- Resultados --------------------
\section{Results}

% ***** "The limitation of considering only 64 of the 300 classes in the dataset must be noted when discussing the findings" *****
\indent This section presents the performance of the autoencoder-classifier and the entropic associative memory as the number of classes varies. Out of the 345 classes in the \emph{Quick, Draw!} dataset, a total of 64 were randomly selected for the experiments, and balanced to 113,613 instances each. The perceptual system was trained using 70\% of this dataset. Experiments were run using the first 2, 4, 8, 16, 24, and 32 classes only, due to memory constraints. For each configuration, 20\% of the corresponding dataset was used as the entropic associative memory's filling corpus, and 10\% as the testing corpus for both the perceptual system and the memory. Due to time constraints, and memory limitations of the server, experiments with more classes were not performed. However, the selection of 64 classes is considered sufficient to observe a clear trend in the results, as their consistency suggests that they are not far from what would be obtained with cross-validation.

% ********************************************************************************************************************************

As in \cite{pinedaImageryEntropicAssociative2023} and \cite{moralesMissingCueProblem2025}, the neural networks implementing the perceptual system, and autoencoder and a classifier, were trained jointly. Table~\ref{tab:autoencoder-classifier-performance} presents their performance on the test set for the different numbers of classes taken from the dataset. As can be observed, the classifier's performance tends to decrease as the number of classes increases, but it remained above 91\% accuracy. The autoencoder's performance is slightly more stable, with a difference of less than 3\% across all cases.

\begin{table}[htbp]
	\centering
	\caption{Performance of the autoencoder (root mean square error) and the classifier (accuracy) for different numbers of classes.}
	\begin{tabular}{@{}ccc@{}}\toprule
			& Classifier & Autoencoder \\
		Number of Classes & Accuracy &  RMSE \\
		\midrule
		2 & 0.976 & 0.218 \\
		4 & 0.950 & 0.215 \\
		8 & 0.921 & 0.204 \\
		16 & 0.914 & 0.218 \\
		24	& 0.917 & 0.228 \\
		32 & 0.917 & 0.229 \\
		\bottomrule
	\end{tabular}
	\label{tab:autoencoder-classifier-performance}
\end{table}

To facilitate a comparison of the results, Table~\ref{tab:summary-performance} provides a summary of the key performance metrics across the two experimental setups; it shows the baseline classifier accuracy alongside the estimated peak accuracy of the EAM in both the recognition and rejection experiments.

% ***** "The presentation of the results can be complemented with a table for easier comparison" *****
\begin{table}[htbp]
	\centering
	\caption{Summary of performance across experiments.}
	\label{tab:summary-performance}
	\begin{tabular}{@{}cccc@{}}\toprule
		Number of & Classifier & EAM Peak Accuracy & EAM Peak Accuracy \\
		Classes & Accuracy & (Recognition Exp.) & (Rejection Exp.) \\
		\midrule
		2 & 0.976 & 0.975 & 0.943 \\
		4 & 0.950 & 0.951 & 0.925 \\
		8 & 0.921 & 0.920 & 0.814 \\
		16 & 0.914 & 0.940 & 0.686 \\
		24	& 0.917 & 0.915 & 0.584 \\
		32 & 0.917 & 0.917 & 0.601 \\
		\bottomrule
	\end{tabular}
\end{table}
% ******************************************************************




\subsection{Recognition Experiment Results}

% ***** "In Figures 1-6, it is not clear what the entropy bar at the bottom of the figures is for. More details or a brief description will be helpful for the reader" *****
\noindent In this experiment, the memory stored all the available classes. The graphs included in Figure~\ref{fig:results_exp1} show the performance of the entropic associative memory using precision and accuracy metrics. In all cases, the same number of memory columns (256) was kept, while the number of rows was varied in powers of two, from 2 up to 1024. Overall, it can be observed that the number of rows needed for the memory to perform well increases rather slowly with the number of classes stored in it. The performances with 4 or more rows were very similar to each other and comparable to those obtained by the classifier.

\begin{figure*}[htbp]
	\centering
	\subfloat[2 classes\label{fig:exp1_2}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_02/graph_prse_MEAN-english}}
	\hfil
	\subfloat[4 classes\label{fig:exp1_4}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_04/graph_prse_MEAN-english}}\\
	\subfloat[8 classes\label{fig:exp1_8}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_08/graph_prse_MEAN-english}}
	\hfil
	\subfloat[16 classes\label{fig:exp1_16}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_16/graph_prse_MEAN-english}}\\
	\subfloat[24 classes\label{fig:exp1_24}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_24/graph_prse_MEAN-english}}
	\hfil
	\subfloat[32 classes\label{fig:exp1_32}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_32/graph_prse_MEAN-english}}
	\caption{Precision, accuracy, and entropy of the associative memory across the recognition experiment. The color bar at the bottom of each graph represents the entropy of the memory, where lower entropy (blue) indicates high certainty, and higher entropy (red) indicates high uncertainty.}
	\label{fig:results_exp1}
\end{figure*}

\subsection{Rejection Experiment Results}
\noindent In this experiment, only the first half of the classes were stored in the memory, but evaluation was performed against the complete set, using the same procedures and the same metrics as in the recognition experiment. The graphs in Figure~\ref{fig:results_exp2} present the results. It can be observed that the memory performs very well with two and four classes, achieving results close to the corresponding ones in the recognition experiment. However, its performance decreases as the number of classes from the filling corpus (and the number of instances of those classes) stored in memory increases.

\begin{figure*}[htbp]
	\centering
	\subfloat[2 classes\label{fig:exp2_2}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_02/graph_prse_MEAN-exp_002-english}}
	\hfil
	\subfloat[4 classes\label{fig:exp2_4}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_04/graph_prse_MEAN-exp_002-english}}\\
	\subfloat[8 classes\label{fig:exp2_8}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_08/graph_prse_MEAN-exp_002-english}}
	\hfil
	\subfloat[16 classes\label{fig:exp2_16}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_16/graph_prse_MEAN-exp_002-english}}\\
	\subfloat[24 classes\label{fig:exp2_24}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_24/graph_prse_MEAN-exp_002-english}}
	\hfil
	\subfloat[32 classes\label{fig:exp2_32}]{\includegraphics[width=0.45\textwidth]{Graficas/runs_32/graph_prse_MEAN-exp_002-english}}
	\caption{Precision, accuracy, and entropy of the associative memory across the rejection experiment. The color bar at the bottom of each graph represents the entropy of the memory, where lower entropy (blue) indicates high certainty, and higher entropy (red) indicates high uncertainty.}
	\label{fig:results_exp2}
\end{figure*}




% -------------------- Discución --------------------
\section{Discussion}
\noindent The experimental results allow for several relevant observations about the behavior of the Entropic Associative Memory (EAM) in more complex scenarios. First, as progressively increasing the number of classes, a general trend of performance stability in recognition, while decreasing performance in rejection, was observed. This behavior is expected, as the domain complexity and pattern variability grow with the number of categories and their instances---it is important to notice that the maximum memory size, with 256 columns and 1024 rows, is only of 0.5 MB, considering two bytes per cell. Second, the rejection experiment showed that the memory was capable of identifying patterns corresponding to classes registered within it and rejecting instances of unregistered ones. This result provides empirical validation for the hypothesis that EAM can operate in open-set recognition scenarios, differentiating between familiar and unfamiliar stimuli. However, cases of false acceptances for instances of unregistered classes were also detected as the number of classes increased. This suggests a need of adjusting threshold parameters ($\iota$,  $\kappa$, and $\xi$ \cite{pinedaImageryEntropicAssociative2023} were all set to their default value of zero for these experiments), and exploring complementary discrimination mechanisms.

Collectively, these findings demonstrate that EAM not only can adapt to more complex domains, but it also offers an effective mechanism for handling novel information. At the same time, they highlight future challenges related to optimizing its parameters and evaluating it in even larger and more heterogeneous domains. It is important to note, however, that these conclusions are based on experiments with only 64 of the 345 available classes, and future work should aim to scale these tests to the entire dataset.


% -------------------- Conclusiones --------------------
\section{Conclusions}

\noindent This work extended the experimental framework of the Entropic Associative Memory (EAM) to evaluate its performance in more complex scenarios and analyze its rejection capacity for unregistered classes. The main contributions can be summarized as follows: (1) we migrated EAM to a wider and more diverse domain using \emph{Quick, Draw!}, with results that confirm the model's ability to scale in contexts with much more classes and greater variability in patterns; (2) a new experiment was designed and implemented, empirically showing how the EAM is capable of issuing a rejection of stimuli corresponding to unseen classes, validating its usefulness in open-set recognition problems; (3) the system was refactored to accept a variable number of classes and different memory sizes, which enables a more versatile environment for exploring more configurations and comparative analysis.

The findings demonstrate that the EAM is an explainable and robust model, capable of combining distributed storage with probabilistic retrieval and rejection mechanisms. These results reinforce its potential as an alternative paradigm within artificial intelligence, particularly in tasks where the ability to handle novel information is critical.

For future work, it is proposed to extend the experiments to even larger and more heterogeneous domains, to optimize the threshold parameters, and investigate the integration of EAM with more advanced deep learning architectures to strengthen both its accuracy and generalization capacity.


% ***** Comentario del reviwer #2 *****
% -------------------- Reproducibility --------------------
\section*{Reproducibility}
\noindent The source code, experimental data, and scripts required to reproduce the results presented in this paper are available in a public repository at \url{https://github.com/FernyBoy/quick.git}.
% *************************************


% -------------------- Referencias --------------------
\bibliographystyle{IEEEtran}
\bibliography{articulo}
\end{document}
